url: "https://logmeetupnyc.github.io"
baseurl: "/"
title: "Second LoG New York Meetup"
# canonifyurls: true
theme: "hugo-conference"

GoogleAnalytics: ""

params:
  # Conference info
  Name: "Second LoG New York Meetup"
  Description: "LoG Meet Up New York"
  Date: "Nov 21st - Nov 22nd" #"Feb 29th - March 1st"
  #Price: "Only XX€" # If your event is free, just comment this line
  Venue: "New York"
  Address: "101 Hudson St"
  City:  "Jersey City"
  State: "New Jersey"
  Images: ["/img/badge.jpg"]
  GoogleMapsKey: "my-secret-maps-key"

  # Active sections on the website to deactivate comment out with '#'
  # you can also change order here and it will reflect on page
  Sections:
    - about
    - registration
    - cfp
    - schedule
    - speakers
    - selected_talks # comment first upload
    - organizers 
    - sponsors
    #- partners
    - location
    - past_events

  # Titles 
  Titles:
    about: "About"
    registration: "Registration"
    cfp: "Call for talks"
    location: "The Venue"
    schedule: "Schedule"
    speakers: "Speakers"
    selected_talks: "Spotlight Talks"
    sponsors: "Sponsors"
    partners: "Partners"
    organizers: "Organizers"
    past_events: "Previous Events"

  # List of Sponsors
  Sponsors:

    - name: "NJIT"
      logo: "/img/njit.png"
      url: "https://cs.njit.edu"



  # The entire schedule
  Schedule:

    - name: "Registration and Reception"
      time: "Thursday 08h30-09h15"

    - name: "Opening NYC LoG"
      time: "Thursday 09h15-09h30"

    - name: "Scalable Active Learning for Protein Design" # Name Surname
      photo: "/img/nathan.png"
      # url: "www.example.org"
      abstract: "We will discuss our approach and general considerations for implementing active learning and design of experiments to iteratively optimize proteins. Our framework is underpinned by robust data pipelines and algorithmic innovations, including novel strategies for building, pre-training, and fine-tuning foundation models for biology. I'll compare and contrast sequence- and structure-based protein design paradigms and discuss opportunities to think beyond these traditional mindsets."
      bio: "Nathan Frey is a Principal Machine Learning Scientist and Group Leader at Prescient Design, Genentech. At Prescient, his group develops and applies machine learning methods to molecular discovery and design. Previously, Nathan was a Postdoctoral Associate at MIT working with the Lincoln Lab Supercomputing and AI groups. He was a National Defense Science & Engineering Graduate Fellow at the University of Pennsylvania, where he obtained his PhD in Materials Science & Engineering. During graduate school, Nathan was an affiliate scientist with the Materials Project at Berkeley Lab."
      company: "Genentech"
      link:
        href: "https://ncfrey.github.io/"
        #text: "@linus"
      presentation:
        title: "Nathan Frey" # Speaker Name
        time: "Thursday 9h30-10h10"

    - name: "Directed Graph Transfomers" # Name Surname
      photo: "/img/giorgos.jpg"
      # url: "www.example.org"
      abstract: "TBA"
      bio: "Dr. Georgios Kollias is a Research Staff Member (RSM) currently working on memory-augmented LLMs and graph NNs at IBM T.J. Watson Research Center, USA. He obtained the BSc in Physics in 2000 and the MSc in Computational Science in 2002 from the University of Athens, Greece, and the PhD in Computer Science from the University of Patras, Greece, in 2009. He then moved to Purdue University, USA and worked as a Postdoctoral Research Associate in the Computer Science Department and the Center for Science of Information till April 2013. Next, he joined IBM Research, USA also holding a research position at IBM Zurich Research Lab, Switzerland (August 2014 - April 2015). His research interests span the areas of graph mining and analytics, parallel, distributed and high-performance computing, numerical linear algebra and matrix computations, big data analytics, problem solving environments, quantum computations, nonlinear dynamical systems and machine learning applications."
      company: "IBM Research"
      link:
        href: "https://www.linkedin.com/in/giorgos-kollias-7663b21b/"
        #text: "@linus"
      presentation:
        title: "Giorgos Kollias" # Speaker Name
        time: "Thursday 10h10-10h50"

    - name: "Coffee Break"
      time: "Thursday 10h50-11h10"

    - name: "TBA" # Name Surname
      photo: "/img/bryan.jpg"
      abstract: "TBA" 
      bio: "Bryan Perozzi is a Research Scientist in Google Research’s Algorithms and Optimization group, where he routinely analyzes some of the world’s largest (and perhaps most interesting) graphs.  Bryan’s research focuses on developing techniques for learning expressive representations of relational data with neural networks.  These scalable algorithms are useful for prediction tasks (classification/regression), pattern discovery, and anomaly detection in large networked data sets. # Bryan is an author of 40+ peer-reviewed papers at leading conferences in machine learning and data mining (such as NeurIPS, ICML, ICLR, KDD, and WWW).  His doctoral work on learning network representations (DeepWalk) was awarded the prestigious SIGKDD Dissertation Award.  Bryan received his Ph.D. in Computer Science from Stony Brook University in 2016, and his M.S. from the Johns Hopkins University in 2011. "
      company: "Google Research"
      link:
        href: "http://www.perozzi.net/"
        #text: "@linus"
      presentation:
        title: "Bryan Perozzi" # Speaker Name
        time: "Thursday 11h10-12h00"

    - name: "Lunch Break and Social(lunch not provided)"
      time: "Thursday 12h00-13h30"


    - name: "Multi-Vector Representations and Embedding-Based Nearest Neighbor Search"   # Name Surname
      photo: "/img/rajesh.jpg"
      # url: "www.example.org"
      abstract: "Perhaps the biggest breakthrough in information retrieval in the 21st century has been the usage of embedding models. Given a dataset D which could consist of images, sentences, videos, or other modalities, these models map each datapoint in D to a vector in high-dimensional Euclidean space, such that similar datapoints (e.g. similar images) are mapped to similar vectors under the Euclidean distance. This transforms a non-mathematical similarity to a mathematical one, reducing information retrieval to Euclidean Nearest Neighbor Search, which has been studied extensively in theory for over three decades. While this paradigm has been extremely successful, the representation of data by a single vector has limitations. In particular, it must compress all aspects of the data into a single global representation. Unfortunately, the actual similarity between two data-points can depend on a complex interaction between multiple "local" features of the data (e.g. words, subregions of an image, subsections of a document, ect.). Beginning with the landmark ColBERT paper (Khattab and Zaharia, SIGIR 2020), this has been addressed by training models to produce multiple embeddings per data-point. To measure the similarity between two sets of vectors A,B, one uses the so-called Chamfer Distance, which is the average distance between each point in A and its nearest point in B. Multi-vector models currently achieve SOTA on many retrieval benchmarks, and are now an extremely popular area of research. However, the usage of the Chamfer Distance raises a host of totally unexplored algorithmic questions. Firstly, Chamfer Distance is more expensive to compute than Euclidean distance. Is it possible to compute it faster, even approximately? Furthermore, to be useful for information retrieval, we need fast nearest neighbor search algorithms for Chamfer, but do such algorithms exist? In this talk, we investigate these algorithmic questions, and explore how techniques from TCS can be applied to expand the possibilities of practical information retrieval."
      bio: "Rajesh is a Research Scientist at Google Research NYC, where he is part of the Algorithms and Optimization Group. Previously, he completed his PhD at Carnegie Mellon University, where he was advised by Professor David P. Woodruff. His research focuses on algorithms for high-dimensional geometry, especially streaming, sketching, and sublinear algorithms. He is the winner of two PODS best paper awards (2019, 2020) for his work on adversarial streaming algorithms and database theory, and has publications spanning a wide variety of other algorithmic domains, including property testing, randomized numerical linear algebra, nearest neighbor search, distributed algorithms, clustering, geometric streaming, and learning theory."
      company: "Google Research"
      link:
        href: "https://rajeshjayaram.com/"
        #text: "@linus"
      presentation:
        title: "Rajesh Jayaram" # Speaker Name
        time: "Thursday 13h30-14h10"

    - name: "Leveraging Structured Knowledge for Generative AI Applications" # Name Surname
      photo: "/img/huzefa.jpeg"
      # url: "www.example.org"
      # abstract: "In this talk, He will be discussing the problem of defining generative models on manifolds. Designing a general recipe for training probabilistic models that works immediately on a wide selection of manifolds has been a difficult challenge, with some naive approaches ignoring the topology of the manifold, suffering from algorithmic complexities, or resulting in biased training objectives. He will introduce his recent work on Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds, which bypasses many inherent limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not suffer from scaling difficulties, and its objective is a simple regression with closed-form target vector fields. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which generalizes the existing Euclidean case. To extend to general geometries, we propose the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries."
      bio: "At AWS AI/ML, Huzefa Rangwala leads a team of scientists and engineers, revolutionizing AWS services through advancements in graph machine learning, reinforcement learning, AutoML, low-code/no-code generative AI, and personalized AI solutions. His passion extends to transforming analytical sciences with the power of generative AI. He is a Professor of Computer Science and the Lawrence Cranberg Faculty Fellow at George Mason University, where he also served as interim Chair from 2019-2020. He is the recipient of the National Science Foundation (NSF) Career Award, the 2014 University-wide Teaching Award, Emerging Researcher/Creator/Scholar Award, the 2018 Undergraduate Research Mentor Award. In 2022, Huzefa co-chaired the ACM SIGKDD conference in Washington, DC. His research interests include structured learning, federated learning, and ML fairness inter-twinned with applying ML to problems in biology, biomedical engineering, and learning sciences"
      company: "Amazon"
      link:
        href: "https://scholar.google.com/citations?user=yWJ9BqEAAAAJ&hl=en"
        #text: "@linus"
      presentation:
        title: "Huzefa Rangwala" # Speaker Name
        time: "Thursday 14h10-14h50"
    
    - name: "Coffee Break"
      time: "Thursday 14h50-15h10"

    - name: "Discrete generative modeling with masked diffusions" # Name Surname
      photo: "/img/jiaxin.jpg"
      abstract: "Modern generative AI has developed along two distinct paths: autoregressive models for discrete data (such as text) and diffusion models for continuous data (like images). Bridging this divide by adapting diffusion models to handle discrete data represents a compelling avenue for unifying these disparate approaches. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this talk, I will introduce masked diffusion models, a simple and general framework that unlock the full potential of diffusion models for discrete data. We show that the continuous-time variational objective of such models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on OpenWebText surpass prior diffusion language models at GPT-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 (CIFAR-10) and 3.40 (ImageNet 64×64) bits per dimension that are better than autoregressive models of similar sizes."
      bio: "Jiaxin Shi is a research scientist at Google DeepMind. Previously, he was a postdoctoral researcher at Stanford and Microsoft Research New England. He obtained his Ph.D. from Tsinghua University. His research interests broadly involve probabilistic and algorithmic models for learning as well as the interface between them. Jiaxin served as an area chair for NeurIPS and AISTATS. He is a recipient of Microsoft Research PhD fellowship. His first-author paper was recognized by a NeurIPS 2022 outstanding paper award." 
      company: "Google DeepMind" 
      link:
        href: "https://jiaxins.io/"
      #  text: "WebPage"
      presentation:
        title: "Jiaxin Shi" #Speaker Name
        time: "Thursday 15h10-15h50"

    - name: "Efficient Approximately Equivariant Neural Networks via Symmetry-Based Structured Matrices" # Name Surname
      photo: "/img/shubhendu.jpeg"
      abstract: "There has been much recent interest in designing symmetry-aware neural networks exhibiting relaxed equivariance. Such NNs aim to interpolate between being exactly equivariant and being fully flexible, affording consistent performance benefits. This talk will present a method to construct approximately equivariant networks that perform roughly at par with the state of the art equivariant networks, but usually operate with one or two orders of magnitude fewer parameters. The construction is based on a novel generalization of classical low-displacement rank theory, which works only for the cyclic group, to general discrete groups and their homogeneous spaces. The generalization allows the design of special structured matrices specific to a group of interest that allows for generalizing equivariant networks, and provide a recipe for controlling approximation error. Ongoing work on efficient tensorizations and GPU implementations will also be discussed."
      bio: "Shubhendu Trivedi works on the theoretical and applied aspects of machine learning. For the past few years his research has focused on developing rigorous theoretical and engineering tools for data-efficient machine learning (e.g. via equivariant and geometric deep learning) and enabling their safe and reliable deployment in real-world applications and decision-making pipelines (e.g. via conformal prediction and provable uncertainty quantification). In research contexts, he has spent time at the MIT CSAIL, Brown University, Fermilab, University of Chicago, TTI-Chicago, WPI, and for industrial work he has been involved with data science consulting in the pharmaceuticals, airlines, and FMCG sectors, and has also spent time at ZS, NEC, United, amongst others. In addition to the above work, he has co-founded a startup in semiconductors and serves on the boards of multiple startups, the most recent of which include Reexpress (LLMs), Brainwell Health (Imaging), and Spark Neuro (EEG)."
        
      company: "" 
      link:
        href: "https://shubhendu-trivedi.org/"
      #  text: "WebPage"
      presentation:
        title: "Shubhendu Trivedi" #Speaker Name
        time: "Thursday 15h50-16h30"

    - name: "Understanding Transformer Reasoning Capabilities via Graph Algorithms" # Name Surname
      photo: "/img/clayton.png"
      # url: "www.example.org"
      abstract: "Which transformer scaling regimes are able to solve different classes of algorithmic problems? Despite the tremendous empirical advances of transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking. We investigate this question in terms of the network's depth, width, and number of extra tokens for algorithm execution. We introduce a novel representational hierarchy that separates graph algorithmic tasks, such as connectivity and shortest path, into equivalence classes solvable by transformers in different realistic parameter scaling regimes. We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We supplement our theoretical analysis with empirical evidence using the GraphQA benchmark. These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks."
      bio: "Clayton Sanford is a research scientist at Google and a recent PhD graduate from Columbia University. In his PhD, he studied machine learning and theoretical computer science under the supervision of Professors Daniel Hsu and Rocco Servedio. His research focuses on developing a more rigorous understanding of neural network expressivity and generalization, with a particular focus on modern language models."
      company: "Google Research"
      link:
        href: "https://claytonsanford.com/"
        #text: "@linus"
      presentation:
        title: "Clayton Sanford" # Speaker Name
        time: "Friday 09h00-09h40"

    - name: "Coffee Break"
      time: "Friday 09h40-10h00"

    - name: "Learning PDEs on General Geometries through Neural Operators with Equivalence" # Name Surname
      photo: "/img/jiequan.png"
      # url: "www.example.org"
      abstract: "Many partial differential equations (PDEs) of interest in science and engineering are strongly shaped by the underlying geometry, with data often represented on unstructured meshes and various coordinate systems in use. These factors create challenges not present in regular domains, like rectangular grids. Despite these complexities, the physics governing these PDEs typically relies on local interactions and remains independent of the coordinate system. In this talk, we will demonstrate how neural operators can be designed to solve PDEs on general geometries by leveraging graph-based methods that incorporate these physics priors. We will also explore various applications, such as learning nonlocal constitutive models and providing surrogate solutions to accelerate convergence toward accurate results." 
      bio: "Jiequn Han is a Research Scientist in the Center for Computational Mathematics, Flatiron Institute, Simons Foundation. He conducts research on machine learning for science, drawing inspiration from various scientific disciplines and focusing on solving high-dimensional problems in scientific computing, primarily those related to PDEs."
      company: "Flatiron Institute" 
      link:
        href: "https://users.flatironinstitute.org/~jhan/"
        #text: "@linus"
      presentation:
        title: "Jiequn Han" # Speaker Name
        time: "Friday 10h00-10h40"


    - name: "TBA" # Name Surname
      photo: "/img/Serina.jpg"
      # url: "www.example.org"
      # abstract: "In this talk, He will be discussing the problem of defining generative models on manifolds. Designing a general recipe for training probabilistic models that works immediately on a wide selection of manifolds has been a difficult challenge, with some naive approaches ignoring the topology of the manifold, suffering from algorithmic complexities, or resulting in biased training objectives. He will introduce his recent work on Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds, which bypasses many inherent limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not suffer from scaling difficulties, and its objective is a simple regression with closed-form target vector fields. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which generalizes the existing Euclidean case. To extend to general geometries, we propose the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries."
      # bio: "Ricky Tian Qi Chen is a Research Scientist at FAIR, Meta based in New York. His research is on building simplified abstractions of the world through the lens of dynamical systems and flows. He generally works on integrating structured transformations into probabilistic modeling, with the goal of improved interpretability, tractable optimization, or extending into novel areas of application."
      company: "Microsoft Research, UC Berkeley"
      link:
        href: "https://serinachang5.github.io/"
        #text: "@linus"
      presentation:
        title: "Serina Chang" # Speaker Name
        time: "Friday 10h40-11h20"

    - name: "Lunch Break and Social(lunch not provided)"
      time: "Friday 11h20-13h00"

    - name: "TBA" # Name Surname
      photo: "/img/alberto.jpg"
      # url: "www.example.org"
      # abstract: "In this talk, He will be discussing the problem of defining generative models on manifolds. Designing a general recipe for training probabilistic models that works immediately on a wide selection of manifolds has been a difficult challenge, with some naive approaches ignoring the topology of the manifold, suffering from algorithmic complexities, or resulting in biased training objectives. He will introduce his recent work on Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds, which bypasses many inherent limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not suffer from scaling difficulties, and its objective is a simple regression with closed-form target vector fields. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which generalizes the existing Euclidean case. To extend to general geometries, we propose the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on many real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries."
      # bio: "Ricky Tian Qi Chen is a Research Scientist at FAIR, Meta based in New York. His research is on building simplified abstractions of the world through the lens of dynamical systems and flows. He generally works on integrating structured transformations into probabilistic modeling, with the goal of improved interpretability, tractable optimization, or extending into novel areas of application."
      company: "Flatiron Institute"
      link:
        href: "https://alberto.bietti.me/"
        #text: "@linus"
      presentation:
        title: "Alberto Bietti" # Speaker Name
        time: "Friday 13h00-13h40"

    - name: "Learning Scalable Diffusion Models using Optimality and Constraint Structures"
      photo: "/img/guan-horng.png"
      # url: ""
      abstract: "Generative AI has made remarkable strides in recent years, largely propelled by the development of diffusion models. However, the successes of diffusion models largely rely on the structure of the data-to-noise diffusion processes, posing challenges for more general distribution matching problems beyond generative modeling. In this talk, I will present some of my recent works on learning scalable diffusion models for general distribution matching. Specifically, I will demonstrate how optimality and constraint structures can serve as a principled way for algorithmic design, generalizing those used in training denoising diffusion models, and achieve superior performance in image restoration, unsupervised image translation, and watermarked image generation."
      bio: "Guan-Horng Liu is a Research Scientist at Fundamental AI Research (FAIR) at Meta in NY. He holds a Ph.D. in Machine Learning from Georgia Tech, where he was supervised by Evangelos Theodorou, and a M.S. in Robotics from Carnegie Mellon University. His research lies in fundamental algorithms for learning diffusion models with optimality structures, with the goals of enhancing theoretical understanding and developing large-scale algorithms for novel applications. His papers have been awarded with spotlight and oral presentations in ICLR, ICML, NeurIPS. He was supported by the Graduate Fellowship from the School of Aerospace Engineering at Georgia Tech."
      company: "Meta" 
      link:
        href: "https://ghliu.github.io/"
        #text: "@linus"
      presentation:
        title: "Guan-Horng Liu" # Speaker Name
        time: "Friday 13h40-14h20"

    - name: "Coffee Break"
      time: "Friday 14h20-14h40"

    - name: "Variational Schrödinger Diffusion Models and Beyond" # Name Surname
      photo: "/img/wei.jpeg"
      # url: "www.example.org"
      abstract: "Schrödinger bridge (SB) methods are crucial for optimizing transportation plans in diffusion models, but they are often costly and complex due to intractable forward score functions. To address these issues and improve scalability, we introduce the Variational Schrödinger Diffusion Model (VSDM) and its momentum extensions. VSDM uses variational inference to simplify the forward score functions, creating a simulation-free forward process for training backward scores. This model leverages stochastic approximation to train its variational scores, balancing accuracy and computational budget. To further enhance transport and scalability, we explore momentum accelerations, which simplify the denoising process and reduce tuning costs. Empirically, both models demonstrate remarkable performance in anisotropic optimal transport and image generation, proving more scalable than classical SB methods without requiring warm-up training. Additionally, the conditional versions show exceptional performance in time series prediction."
      bio: "Wei Deng is a machine learning researcher at Morgan Stanley, NY. He completed his Ph.D. from Purdue University in 2021. His research focuses on Monte Carlo methods, Diffusion Models, and State Space Models. His objective is to develop more scalable and reliable probabilistic methods for solving machine learning applications in Bayesian inference, generative models, and time series. His works are mainly published in machine learning conferences, such as NeurIPS, ICML, ICLR, UAI, AISTATS and journals such as Statistics and Computing and Journal of Computational and Graphical Statistics. "
      company: "Morgan Stanley"
      link:
        href: "https://www.weideng.org/"
        #text: "@linus"
      presentation:
        title: "Wei Deng" # Speaker Name
        time: "Friday 14h40-15h20"


    - name: "Towards Graph Transformers at Scale" # Name Surname
      photo: "/img/qitian.jpeg"
      abstract: "Representation learning over graph-structured data is a long-standing fundamental challenge in learning on graphs. Recent advances have demonstrated the promise of Transformer-style models that leverage global all-pair attentions for graph representation learning. However, the quadratic complexity and complicated architectures play as two computational bottlenecks for Transformers in large-scale data, and beyond accuracy criteria, it lacks principled guidance for interpretable model designs. This talk will introduce three recent works that present a scalable graph Transformer with linear complexity (NodeFormer), a simplified Transformer with single-layer global propagation (SGFormer), and an energy-constrained diffusion framework that reveals the inherent connections between graph neural networks and Transformers (DIFFormer). Future directions will be discussed at the end of this talk."
      bio: "Qitian Wu is a postdoctoral fellow at Broad Institute of MIT and Harvard. Prior to this, he achieved PhD in Computer Science at Shanghai Jiao Tong University. His research interest focuses on machine learning with complex structured data. His recent works endeavor to develop efficient foundational backbones for representing large-scale graph data and provably generalizable learning algorithms for handling distribution shifts. He also seek to apply this methodology to address the pressing problems in recommender systems and biomedical science. He is the recipient of Eric and Wendy Schmidt Center Fellowship, Microsoft Research PhD Fellowship, and Baidu PhD Scholarship." 
      company: "Broad Institute of MIT and Harvard" 
      link:
        href: "https://qitianwu.github.io/"
      #  text: "WebPage"
      presentation:
        title: "Qitian Wu" #Speaker Name
        time: "Friday 15h20-16h00"



















#     - name: "Hyperbolic Brain Network Representations for Subjective Cognitive Decline
# Prediction and Detecting Healthy Brain Aging Trajectories" # Name Surname
#       photo: "/img/mengjia.png"
#       abstract: "An expansive area of research focuses on discerning patterns of alterations in
# functional brain networks from the early stages of Alzheimer’s disease, even at the subjective
# cognitive decline (SCD) stage. Here, we developed a novel hyperbolic MEG brain network
# embedding framework for transforming complex MEG brain networks into lower-dimensional
# hyperbolic representations. Using this model, we computed hyperbolic embeddings of the MEG
# brain networks of two distinct participant groups: individuals with SCD and healthy controls. We
# demonstrated that these embeddings preserve both local and global geometric information,
# presenting reduced distortion compared to rival models, even when brain networks are mapped
# into low-dimensional spaces. In addition, our findings showed that the hyperbolic embeddings
# encompass unique SCD-related information that improves the discriminatory power above and
# beyond that of connectivity features alone. Overall,
# this study presents the first evaluation of hyperbolic embeddings of MEG brain networks,
# offering novel insights into brain organization, cognitive decline, and potential diagnostic
# avenues of Alzheimer’s disease."
#       bio: "Mengjia Xu is currently an Assistant Professor at Department of Data Science, Ying Wu College of Computing, NJIT. She also holds a Research Affiliate position with the MIT NSF Center for Brains, Minds, and Machines (CBMM) at McGovern Institute for Brain Research. "
        
#       company: "New Jersey Institute of Technology" 
#       link:
#         href: "https://gracexu182.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Mengjia Xu" #Speaker Name
#         time: "Thursday 14h10-14h50"

#     - name: "Coffee Break"
#       time: "Thursday 14h50-15h10"

#     - name: "Graph Ricci Flow and Applications in Network Analysis and Learning" # Name Surname
#       photo: "/img/jie.jpg"
#       abstract: "The notion of curvature describes how spaces are bent at each point and Ricci flow deforms the space such that curvature changes in a way analogous to the diffusion of heat. In this talk I will discuss some of our work on discrete Ollivier Ricci curvature defined on graphs. Discrete curvature defined on an edge captures the local connectivity in the neighborhood. In general edges within a densely connected community have positive curvature while edges connecting different communities have negative curvature. By deforming edge weights with respect to curvature one can derive a Ricci flow metric which is robust to edge insertion/deletion. I will present applications of graph Ricci flow in graph analysis and learning, including network alignment, community detection and graph neural networks."
#       bio: "Jie Gao is a Professor of Computer Science department of Rutgers University. From 2005-2019 she was on faculty of Department of Computer Science, Stony Brook University.
#       Her reserach is in the intersection of Algorithm Design, Computational Geometry and Networking applications such as wireless, mobile, and sensor networks, and more recently social networks, trajectory data/privacy, and scheduling problems in robotics and networking."
        
#       company: "Rutgers University" 
#       link:
#         href: "https://sites.rutgers.edu/jie-gao/"
#       #  text: "WebPage"
#       presentation:
#         title: "Jie Gao" #Speaker Name
#         time: "Thursday 15h10-15h50"

#     - name: "Learning the laws and composition of the Universe with cosmic graphs" # Name Surname
#       photo: "/img/paco.png"
#       abstract: "Cosmology is a branch of astrophysics dedicated to the study of the laws and constituents of the Universe. To achieve this, cosmologists look at the spatial distribution of galaxies in the Universe with the goal of finding patterns in that distribution that reveal the fundamental physics behind the dynamics of the cosmos. In this talk, I will show how deep learning is revolutionizing the way cosmologists tackle decades-old problems and how graph neural networks can be used in combination with state-of-the-art hydrodynamic simulations to maximize the amount of information that can be extracted from cosmological observations."
#       bio: "Francisco (Paco) Villaescusa-Navarro is a research scientist at the Flatiron Institute in New York City. He did his PhD at the University of Valencia in Spain. He held postdoctoral positions at the Astronomical Observatory of Trieste, Italy, and the Center for Computational Astrophysics in New York before becoming an associate research scholar at Princeton University, where he holds a visiting research scholar position. Paco is the main architect of the Quijote simulations, the largest suite of cosmological N-body simulations ever run. He is also part of the CAMELS core team that designed and ran the largest set of state-of-the-art hydrodynamic simulations to date. Paco combines the output of numerical simulations with machine learning methods to develop theoretical models to extract the maximum amount of information from cosmological surveys in order to unveil the Universe’s mysteries."
#       company: "Flatiron Institute" 
#       link:
#         href: "https://franciscovillaescusa.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Francisco Villaescusa-Navarro" #Speaker Name
#         time: "Thursday 15h50-16h30"

    
#     - name: "End-to-end learning geometries for graphs, dynamical systems, and regression" # Name Surname
#       photo: "/img/brandon.png"
#       abstract: "Every machine learning setting has an underlying geometry where
# the data is represented and the predictions are performed in.
# While defaulting the geometry to a Euclidean or known manifold is
# capable of building powerful models, /learning/ a non-trivial geometry
# from data is useful for improving the overall performance and estimating
# unobserved structures.
# This talk focuses on learning geometries for:
# 1) *graph embeddings*, where the geometry of the embedding,
# (e.g., Euclidean, spherical, or hyperbolic) heavily influences the
# accuracy and distortion of the embedding;
# 2) *dynamical systems*, where the geometry of the state space can uncover
# unobserved properties of the underlying systems, e.g.,
# geographic information such as obstacles or terrains; and
# 3) *regression*, where the geometry of the prediction space
# influences where the model should be accurate or inaccurate
# for some downstream task.
# We will focus on /latent/ geometries in these settings that are
# not directly observable from the data, i.e., the geometry cannot
# be estimated as a submanifold of the Euclidean space the data
# is observed in.
# Instead, the geometry can be shaped via a downstream
# signal that propagates through differentiable operations such as
# the geodesic distance, and log/exp maps on Riemannian manifolds.
# The talk covers the foundational tools here on making operations
# differentiable (in general via the envelope and implicit function theorems,
# and simpler when closed-form operations are available),
# and demonstrates where the end-to-end learned geometry is effective."
#       bio: "Brandon Amos is a Research Scientist in Meta AI’s Fundamental AI
# Research group in NYC. He holds a PhD in Computer Science from
# Carnegie Mellon University and was supported by the USA National
# Science Foundation Graduate Research Fellowship (NSF GRFP). Prior to
# joining Meta, he has worked at Adobe Research, Google DeepMind,
# and Intel Labs. His research interests are in machine learning and
# optimization with a recent focus on reinforcement learning, control,
# optimal transport, and geometry."
#       company: "Meta AI (FAIR)" 
#       link:
#         href: "https://bamos.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Brandon Amos" #Speaker Name
#         time: "Thursday 16h30-17h00"

#     - name: "Spotlight Talk"
#       time: "Thursday 17h00-17h15"

#     - name: "Spotlight Talk"
#       time: "Thursday 17h15-17h30"


#     ##### Friday
#     - name: "The Next Generation of AI and Data Science in Computational Health: A Full-Stack Holistic Perspective" # Name Surname
#       photo: "/img/fei.jpeg"
#       abstract: "With the revolution of machine learning technologies in recent years, AI and data science are holding greater promise in understanding diseases and improving quality of care. Computational health is such a research area aiming at developing computational methodologies for deriving insights from various biomedical data. Currently the research in computational health has been mostly siloed, with different communities focusing on analyzing different types of data. However, human health has its own ecosystem with information from all aspects including genome, phenome and exposome. We need to integrate the insights from all of them to have more holistic understandings of diseases. In this talk, I will present the research from my lab health in recent years on building machine learning models for analyzing different types of data involved in different levels of human life science, and the need for transitioning from conventional focused-community based strategy to a holistic full-stack regime for the next-generation health AI/data science research."
#       bio: "Fei Wang is a Professor in Division of Health Informatics, Department of Population Health Sciences, Weill Cornell Medicine (WCM), Cornell University. He is also the founding director of the WCM institute of AI for Digital Health (AIDH). His major research interest is AI and digital health. He has published more than 350 papers on the top venues of related areas such as ICML, KDD, NIPS, CVPR, AAAI, IJCAI, Nature Medicine, JAMA Internal Medicine, Annals of Internal Medicine, Lancet Digital Health, etc. His papers have received over 29,000 citations so far with an H-index 81. His (or his students’) papers have won 8 best paper (or nomination) awards at top international conferences on data mining and medical informatics. His team won the championship of the AACC PTHrP result prediction challenge in 2022, NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson's Progression Markers' Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, as well as the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang also received prestigious industry awards such as the Sanofi iDEA Award (2021), Google Faculty Research Award (2020) and Amazon AWS Machine Learning for Research Award (2017, 2019 and 2022). Dr. Wang’s Research has been supported by a diverse set of agencies including NSF, NIH, ONR, PCORI, MJFF, AHA, etc. Dr. Wang is the past chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA). Dr. Wang is a fellow of AMIA, a fellow of IAHSI, a fellow of ACMI and a distinguished member of ACM. "
#       company: "Weill Cornell Medicine" # Cat Foundation
#       link:
#         href: "https://wcm-wanglab.github.io/index.html"
#       #  text: "WebPage"
#       presentation:
#         title: "Fei Wang" #Speaker Name
#         time: "Friday - 09h00-09h40"


#     - name: "Coffee Break"
#       time: "Friday 9h40-10h00"

#     - name: "Keynote Talk" # Name Surname
#       photo: "/img/vahab.jpg"
#       abstract: "TBA"
#       bio: "Vahab Mirrokni is a Google Fellow and VP at Google Research, leading algorithm and optimization research groups at Google. These research teams include: market algorithms, large-scale graph mining, and large-scale optimization. Previously he was a distinguished scientist and senior research director at Google. He received his PhD from MIT in 2005 and his B.Sc. from Sharif University of Technology in 2001. He joined Google Research in 2008, after research positions at Microsoft Research, MIT and Amazon.com. He is the co-winner of best paper awards at KDD, ACM EC, and SODA. His research areas include algorithms, distributed and stochastic optimization, and computational economics. Recently he has been working on various algorithmic problems in machine learning, online optimization and mechanism design, and large-scale graph-based learning ."
#       company: "Google Research" # Cat Foundation
#       link:
#         href: "https://people.csail.mit.edu/mirrokni/Welcome.html"
#       #  text: "WebPage"
#       presentation:
#         title: "Vahab Mirrokni " #Speaker Name
#         time: "Friday - 10h00-11h00"

#     # - name: "Keynote Talk"
#     #   time: "Friday 09h40-10h40"

#     # - name: "Coffee Break"
#     #   time: "Friday 10h40-11h00"

#     - name: "LATENTDOCK: Protein-Protein Docking with Latent Diffusion" # Name Surname
#       photo: "/img/matt.jpg"
#       abstract: "Interactions between proteins form the basis for many biological processes, and understanding their relationships is an area of active research. Computational approaches offer a way to facilitate this understanding without the burden of expensive and time-consuming experiments. Here, we introduce LATENTDOCK, a generative model for protein-protein docking. Our method leverages a diffusion model operating within a geometrically-structured latent space, derived from an encoder producing roto-translational invariant representations of protein complexes. Critically, it is able to perform flexible docking, capturing both backbone and side-chain conformational changes. Furthermore, our model can condition on binding sites, leading to significant performance gains. Empirical evaluations show the efficacy of our approach over relevant baselines, even outperforming models that do not account for flexibility."
#       bio: "Matthew McPartlon is a senior research scientist at VantAI where he works on deep learning models for predicting protein-protein and protein-small molecule interactions. Prior to joining VantAI he completed his PhD in computer science at The University of Chicago under Jinbo Xu. His current research is focused on learning invariant representations of 3D geometry and applying this to structure related problems in biology."
#       company: "VantAI" # Cat Foundation
#       link:
#         href: ""
#       #  text: "WebPage"
#       presentation:
#         title: "Matthew McPartlon" #Speaker Name
#         time: "Friday - 11h00-11h40"

#     - name: "Lunch Break and Social(lunch not provided)"
#       time: "Friday - 11h40-13h00"

#     - name: "Luca Naef(VantAI), Samuel Stanton(Genetech), Anton Tsitsulin(Google Research), Vassilis Kalantzis(IBM Reserach), Soledad Villar(JHU) " # Name Surname
#       photo: ""
#       # abstract: "Panel Discussion"
#       # bio: ""
#       company: "" # Cat Foundation
#       link:
#         href: ""
#       #  text: "WebPage"
#       presentation:
#         title: "Panel Discussion" #Speaker Name
#         time: "Friday - 13h00-14h00"

#     # - name: ""
#     #   time: ""

#     - name: "Promises and Pitfalls of Geometric Deep Learning for Protein Design" # Name Surname
#       photo: "/img/samuel.jpg"
#       abstract: "Protein design appears to be an ideal application of geometric deep learning, with well-motivated symmetries and inductive biases from physics and structural biology. Indeed, until recently in silico protein design was completely dominated by hand-crafted structural models. Recent successes with sequence models are challenging this paradigm, but many researchers still believe strong geometric inductive biases are essential for generalization in data-scarce regimes. We will review a pragmatic assessment of the strengths and weaknesses of structural and sequence-based methods, and discuss the greatest opportunities and challenges facing structural methods in particular in contemporary protein design problems."
#       bio: "Samuel Stanton is a Machine Learning Scientist at Genentech, working on ML-driven drug discovery with the Prescient Design team. Prior to joining Genentech, Samuel received his PhD from the NYU Center for Data Science as an NDSEG fellow under the supervision of Dr. Andrew Gordon Wilson. Samuel’s recent work includes core contributions to Genentech’s “lab-in-the-loop” active learning system for molecule lead optimization, as well as basic research on uncertainty quantification and decision-making with machine learning."
#       company: "Genetech" # Cat Foundation
#       link:
#         href: "https://samuelstanton.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Samuel Stanton" #Speaker Name
#         time: "Friday - 14h00-14h40"

#     - name: "Coffee Break"
#       time: "Friday 14h40-15h00"
      
#     - name: "A gentle introduction to causal discovery and local causal discovery" # Name Surname
#       photo: "/img/kyra.png"
#       abstract: "Causal discovery is crucial for understanding the data-generating mechanism and the causal relationships between variables. Additionally, it has an important application in causal inference in the context of observational studies: it can enable the identification of valid adjustment sets (VAS) for unbiased effect estimation. In this talk, I will introduce graphical models as well as some classical constraint-based causal discovery algorithms. We will see how these methods are notoriously expensive in the nonparametric setting, with exponential time and sample complexity in the worst case. Finally, I will introduce our recent work Local Discovery by Partitioning, a local causal discovery method that is tailored towards VAS identification."
#       bio: "Kyra Gan is an Assistant Professor of Operations Research and Information Engineering at Cornell Tech. Prior to joining Cornell, She was a postdoctoral fellow in the Department of Harvard Statistics, working with Susan Murphy. She obtained her Ph.D. degree in Operations Research from the Tepper School of Business, Carnegie Mellon University in May 2022. Prior to CMU, She received my BA degrees in Mathematics (with the Ann Kirsten Pokora Prize) and Economics from Smith College in May 2017. Her research interests include adaptive/online algorithm design in personalized treatment (including micro-randomized trials and N-of-1 trials) under constrained settings, robust and efficient inference and causal discovery methods."
#       company: "Cornell Tech" # Cat Foundation
#       link:
#         href: "https://kyra-gan.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Kyra Gan" #Speaker Name
#         time: "Friday - 15h00-15h40"

#     - name: "Capturing graph directionality using  transformers" # Name Surname
#       photo: "/img/me.jpg"
#       abstract: "Most existing graph transformers typically capture distances between graph nodes and do not
# take edge direction into account. This is a limiting assumption since many graph applications
# need to exploit sophisticated relationships in graph data, such as time, causality, or generic
# dependency constraints. We introduce a novel graph transformer architecture that explicitly
# takes into account the directionality between connected graph nodes. To achieve this, we make
# use of dual encodings to represent both potential roles, i.e., source or target, of each pair
# of vertices linked by a directed edge. These encodings are learned by leveraging the latent
# adjacency information extracted from a directional attention module, localized with $k$-hop
# neighborhood information. Extensive experiments on synthetic and real graph datasets show that
# our approach can lead to accuracy gains. We also discuss an exploration in mapping directed
# graph transformers to quantum computers."
#       bio: "Vasileios Kalantzis is a Senior Research Scientist at IBM T.J. Watson Research Center, NY. Prior to his current role he was a Herman H. Goldstine Memorial Postdoctoral fellow (2018-2019) and Staff Research Scientist (2019-2023). Vasileios obtained his PhD (2018) and MSc (2016) in Computer Science and Engineering from the University of Minnesota, Twin Cities, and his MSc (2014) and MEng (2011) in Computer Engineering from the University of Patras, Greece. His research interests lie in topics in numerical linear algebra, high-performance computing, graph analytics, and randomized algorithms."
#       company: "IBM Research" # Cat Foundation
#       link:
#         href: "https://vkalantzi.github.io/"
#       #  text: "WebPage"
#       presentation:
#         title: "Vassilis Kalantzis" #Speaker Name
#         time: "Friday - 15h40-16h20"

#     # - name: "The Next Generation of AI and Data Science in Computational Health: A Full-Stack Holistic Perspective" # Name Surname
#     #   photo: "/img/fei.jpeg"
#     #   abstract: "With the revolution of machine learning technologies in recent years, AI and data science are holding greater promise in understanding diseases and improving quality of care. Computational health is such a research area aiming at developing computational methodologies for deriving insights from various biomedical data. Currently the research in computational health has been mostly siloed, with different communities focusing on analyzing different types of data. However, human health has its own ecosystem with information from all aspects including genome, phenome and exposome. We need to integrate the insights from all of them to have more holistic understandings of diseases. In this talk, I will present the research from my lab health in recent years on building machine learning models for analyzing different types of data involved in different levels of human life science, and the need for transitioning from conventional focused-community based strategy to a holistic full-stack regime for the next-generation health AI/data science research."
#     #   bio: "Fei Wang is a Professor in Division of Health Informatics, Department of Population Health Sciences, Weill Cornell Medicine (WCM), Cornell University. He is also the founding director of the WCM institute of AI for Digital Health (AIDH). His major research interest is AI and digital health. He has published more than 350 papers on the top venues of related areas such as ICML, KDD, NIPS, CVPR, AAAI, IJCAI, Nature Medicine, JAMA Internal Medicine, Annals of Internal Medicine, Lancet Digital Health, etc. His papers have received over 29,000 citations so far with an H-index 81. His (or his students’) papers have won 8 best paper (or nomination) awards at top international conferences on data mining and medical informatics. His team won the championship of the AACC PTHrP result prediction challenge in 2022, NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson's Progression Markers' Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, as well as the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang also received prestigious industry awards such as the Sanofi iDEA Award (2021), Google Faculty Research Award (2020) and Amazon AWS Machine Learning for Research Award (2017, 2019 and 2022). Dr. Wang’s Research has been supported by a diverse set of agencies including NSF, NIH, ONR, PCORI, MJFF, AHA, etc. Dr. Wang is the past chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA). Dr. Wang is a fellow of AMIA, a fellow of IAHSI, a fellow of ACMI and a distinguished member of ACM. "
#     #   company: "Weill Cornell Medicine" # Cat Foundation
#     #   link:
#     #     href: ""
#     #   #  text: "WebPage"
#     #   presentation:
#     #     title: "Fei Wang" #Speaker Name
#     #     time: "Friday - 15h40-16h20"

#     - name: "Poster Session"
#       time: "Friday 16h20-17h00"
    
#     - name: "Closing Remarks"
#       time: "Friday 17h00-17h15"


  # selected_talks:

  #   - slot: "Thursday 17h00-17h15"
  #     title: "Rethinking Symmetries in Graph Neural Networks"
  #     speaker: "Teresa Huang"

  #   - slot: "Thursday 17h15-17h30"
  #     title: "Network Formation and Dynamics Among Multi-LLMs"
  #     speaker: "Marios Papachristou"


  # List of Contacts
  Contacts:

    - name: "Ioannis Koutis"
      logo: "/img/IK.jpg"
      email: "ikoutis@njit.edu"
      affiliation: "New Jersey Institute of Technology"
      #url: "https://gohugo.io"
      home: "https://web.njit.edu/~ikoutis/" 
      linkedin: "https://www.linkedin.com/in/yiannis-koutis-08b3a84" 
      googleScholar: "https://scholar.google.com/citations?user=ONXC8_gAAAAJ&hl=en" 

    - name: "Vincent Oria"
      logo: "/img/vincent.jpg"
      email: "vincent.oria@njit.edu"
      affiliation: "New Jersey Institute of Technology"
      #url: "https://gohugo.io"
      home: "https://web.njit.edu/~oria/" 
      linkedin: "https://www.linkedin.com/in/vincent-oria-7b06a114/" 
      googleScholar: "https://scholar.google.com/citations?hl=en&user=PlgV6FwAAAAJ&view_op=list_works&sortby=pubdate" 

    - name: "Ali Parviz"
      logo: "/img/Ali1.jpg"
      email: "ap2248@njit.edu"
      affiliation: "New Jersey Institute of Technology"
      # home: "https://mikelhernaez.github.io/" 
      linkedin: "https://www.linkedin.com/in/alipokhbe/" 
      googleScholar: "https://scholar.google.com/citations?user=GU1VHgMAAAAJ&hl=en" 

    - name: "Yuanqi Du"
      logo: "/img/yuanqi.jpg"
      email: "yuanqidu@cs.cornell.edu"
      affiliation: "Cornell University"
      # url: "https://yuanqidu.github.io/"
      home: "https://yuanqidu.github.io/"
      linkedin: "https://www.linkedin.com/in/yuanqi-du-523968148" 
      googleScholar: "https://scholar.google.com/citations?user=fAc_zZMAAAAJ&hl=en" # Add the Google Scholar URL here 


    - name: "Yingheng Wang"
      logo: "/img/yingheng.jpg"
      email: "yingheng@cs.cornell.edu"
      affiliation: "Cornell University"
      # url: "isjakewong.github.io"
      home: "https://isjakewong.github.io"
      linkedin: "https://www.linkedin.com/in/yingheng-w-7b8066171/" 
      googleScholar: "https://scholar.google.com/citations?user=4WEa7tMAAAAJ&hl=en"

    - name: "Derek Lim"
      logo: "/img/dereck.jpg"
      email: "dereklim@mit.edu"
      affiliation: "MIT"
      # url: "cptq.github.io"
      home: "cptq.github.io"
      linkedin: "https://www.linkedin.com/in/lim-derek/" 
      googleScholar: "https://scholar.google.com/citations?user=y9YTBIsAAAAJ&hl=en"

    - name: "Jacqueline Maasch"
      logo: "/img/jacq.png"
      email: "maasch@cs.cornell.edu"
      affiliation: "Cornell Tech"
      # url: 
      home: "https://jmaasch.github.io/"
      linkedin: "https://www.linkedin.com/in/jmaasch/"
      googleScholar: "https://scholar.google.com/citations?user=5l9n9J8AAAAJ&hl=en&authuser=1"

    - name: "Yanbang Wang"
      logo: "/img/yangbang.jpg"
      email: "ywangdr@cs.cornell.edu"
      affiliation: "Cornell University"
      url: ""
      home: "https://www.cs.cornell.edu/~ywangdr/"
      linkedin: "https://www.linkedin.com/in/ywangdr/" 
      googleScholar: "https://scholar.google.com/citations?user=Ch3YUgsAAAAJ&hl=en"

  # Team:
    # - name: "Yingheng"
    # - name: "Dereck Lim"

  past_events:

    - slot: abc


